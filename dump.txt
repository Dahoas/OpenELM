You are responsible for designing a decision policy to solve the following task: 
You are an agent in 2-D gridworld. You need to forage for food and water, find shelter to sleep, defend against monsters, collect materials, and build tools.
        

        You are allowed a budget of 1M environmnent steps and are evaluated by their success rates of the 23 achievements and by their geometric mean score. 


You will write a python `Policy()`, which should be initializable without any parameters from the user, object which has two methods:
- `def act(observation)` which takes in an observation and returns an action.
- `update(observation, action, reward, next_observation)` which takes in the current observation, chosen action, reward, and next_observation and updates any persistent memory/state between observations.
- `notes: list[str]` which is a list of signals tracked by the policy during execution. The signals collected should be designed by you to improve future iterations of this policy.
- `produce_report() -> str` which uses the collected notes to produce a few line summary you will receive on the policy's performance. You should try to use this report to collect statistics understanding how your policy fails or can be improved. In particular you should ensure you understand the dynamics of the environment, i.e. if the actions you take actually result in the state you expect.
Note: You should not assume any exploration outside of what is learned during the agent's single rollout in the environment. This means you should not rely on Q-learning, etc.

The observation space is defined formally as: 
You can only see a (8, 8) square around the player. Additional information at any given moment is stored in the Player class. 
`
Player.pos: List[int] of length 2 storing x and y coordinates.
Player.facing: Tuple[int,int]. Indicates direction that the player is facing:  

        (-1, 0): 'player-left',
        (+1, 0): 'player-right',
        (0, -1): 'player-up',
        (0, +1): 'player-down',

Player.action: noop
Player.inventory: Dict[str,int] keeping track of the 22 achievements. 

{'health': 9, 'food': 9, 'drink': 9, 'energy': 9, 'sapling': 0, 'wood': 0, 'stone': 0, 'coal': 0, 'iron': 0, 'diamond': 0, 'wood_pickaxe': 0, 'stone_pickaxe': 0, 'iron_pickaxe': 0, 'wood_sword': 0, 'stone_sword': 0, 'iron_sword': 0}
Player.achievements: {'collect_coal': 0, 'collect_diamond': 0, 'collect_drink': 0, 'collect_iron': 0, 'collect_sapling': 0, 'collect_stone': 0, 'collect_wood': 0, 'defeat_skeleton': 0, 'defeat_zombie': 0, 'eat_cow': 0, 'eat_plant': 0, 'make_iron_pickaxe': 0, 'make_iron_sword': 0, 'make_stone_pickaxe': 0, 'make_stone_sword': 0, 'make_wood_pickaxe': 0, 'make_wood_sword': 0, 'place_furnace': 0, 'place_plant': 0, 'place_stone': 0, 'place_table': 0, 'wake_up': 0}

Player.sleeping: bool. 
Player._last_health: int 
Player._hunger: int
Player._thirst: int
Player._fatigue: int
Player._recover: int
Player.health: int


The action space is defined formally as:
action: int such that
- 0  noop

- 1  move_left

- 2  move_right

- 3  move_up

- 4  move_down

- 5  do

- 6  sleep

- 7  place_stone

- 8  place_table

- 9  place_furnace

- 10 place_plant

- 11 make_wood_pickaxe

- 12 make_stone_pickaxe

- 13 make_iron_pickaxe

- 14 make_wood_sword

- 15 make_stone_sword

- 16 make_iron_sword



The rewards are defined formally as:
The sparse reward is +1 for unlocking an achievement during the episode and -0.1 or +0.1 for lost or regenerated health points.


Consider the following example action sequence to familiairize yourself with the env dynamics




You are allowed to use any python library you want but should not assume access to any other external resources (such as models with downloadable weights) unless otherwise specified. In particular you can assume access to the following APIs: 

You should only write the Policy class and nothing else. You are encouraged to be as creative as possible, do not simply copy one of the exemplars if given. Your policy should also be robust to adversity. If it finds itself getting stuck, repeating the same moves, it should try something new. Exploration is encouraged, but keep in mind the goal and the action preconditions.Make sure when you are engaging in exploration, it is diverse and incentivizing discovery of new states.
Thank carefully about the order of steps you propose. Executing the right steps in the wrong order will be costly
All code should be written in a single, large code block.
