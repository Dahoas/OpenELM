# Notes

## Trajectory Grounding Notes

Important to break apart prompting into distinct phases. Don't make model do too much at any time.

Trajectory analysis:
0. Given set of reports could brainstom set of hypotheses for what might be wrong. Then choose one to investigate
1. a) Given set of reports construct hypothesis for what is wrong.
1. b) Given set of reports suggest new reports to better understand what is wrong (if no plausible hypothesis)
2. Propose new reports to validate hypothesis.
3. a) If hypothesis seems correct propose update to given policy
3. b) If hypothesis is wrong choose new hypothesis to try

Might also need a way of validating reports (these can be implemented incorrectly)
- how to get gpt4 to implement reports of varying complexity?
    - currently the metric operates at the transition level with a persistent memory passed among the entire trajectory's computation
- can also decouple report proposal phase from report implementation phase

In general the goal is to provide as much boiler plate as possible and give gpt4
a structured framework in which to work

Instead of using discoveries for refinement could also use them as a guide for conditioning future policies generated by turbo
- this would be more cost effective

Paradigm:
- gpt4 for the analysis
    - we really want the analysis to be reliable
- gpt3.5 for the generation

Moral:
- it's ok to offload discovery
- it's never ok to offload understanding

Exploration is *literally* meaningless without interpretability/understanding. 
- paradoxically, exploration needs to be grounded in prior experience to give deeper meaning

A signal is much more information rich when combined with 10 other signals
- is there such as thing as conditional mutual information? surely

Maybe have gpt-4 propose metric as a reward which tracks whether a policy passes the metric?
- improving reward functions on the spot

This is pivoting away from refinement (but I think that's ok because refinement seems kinda hard)
- not great for our setup since refinement would be expensive
- though should benchmark gpt-turbo's refinement capabilities

GPT-4 feedback can include:
- both a textual instruction
- and a scalar reward indicating when desired behavior is achieved

gpt-3.5 had weird failure case where it thought it could take actions in the update function

Maybe should encourage gpt to write policies in terms of skill composition. e.g. it will be in "time to put down key mode" at some point.
- modularization also makes it easier to give more easily actionable feedback

gpt-4 does tend to get distracted if too many metrics are present. Can try
- presenting with initial metric set
- allowing it to retrieve more issue related metrics afterwards
- or allowing it to propose it's own new metrics

probably not even necessary to include the code policy in the gpt-4 analysis prompt

Our approach is also demonstrating that designing a good/reliable critique is more important than designing a good solution.

Need to encourage both gpt-3.5 and gpt-4 to rely more on the obs, next_obs to verify how the state has changed

Another idea is to have gpt4 define an "effective state" and have it observe that sequence of obs, actions

Idea the policy improvement critique is as specific as possible

Maybe asking gpt-4 to look closely at the source code after identifying a problem is a good idea

Feeding gpt-4 a full trajectory with a compressed state just doesn't work. Too much info.

Faulty report functions are just so bad/misleading. Maybe need a way to unittest them?

Maybe there should be an initial environmental grounding phase which allows gpt-4 to learn basic environment dynamics not communicated in text
- e.g. can only hold one item and cannot drop items on top of each other
- understanding dynamics are necessary for good planning
- discovering unstated preconditions

## Chess Notes

Stockfish takes the most time by far
- (perhaps I really should not be using it to model opponent behavior?)

20 seconds for a 100 move game
- stockfish is not the issue anymore
- main culprits are deepcopy and pushing moves
    - actually it's mostly deepcopy

Playing with stockfish level one and surface level value functions takes 7 seconds for 30 moves (15 rounds)

I don't think mcts properly deals with situations where you arrive at the same state from two different action sequences

mcts is perhaps much less effective in environments with lots of stochasticity
- but it also seems like you can change an environment with stochasticity into a determinsitc one by treating the 
stochasticity as controlled by some opponent (kind of like in chess)
- I guess it is not quite the same actually, since sometimes stochasticity is honest to god random and there's no opponent
- two main options:
    - closed loop: even in these cases it's recommended you just incorporate these evenets directly into your tree as "chance nodes"
        - better with lots of computation
    - open loop: instead of nodes representing states they now represent sequences of actions
        - better with more limited computation

<https://ai.stackexchange.com/questions/22914/how-to-run-a-monte-carlo-tree-search-mcts-for-stochastic-environment>

## 2/12

Next steps.
- chess just doesn't seem to be a good domain unfortunately. Too slow to evaluate and a trivial value function seems to work fairly well
    - also just doing more search seems like the best option
- still want to implement recursive code construction and feedback prompting
- just don't have a good idea of a compelling domain

What is a task that can be compactly represented in code which we can try optimizing for?
- neural network training is a good choice
- maybe something kaggle related is another good choice?
- how did yue's thing work/get past low level action issues? Maybe just need to use gpt4?
    - but even gpt4 was struggling
