You are responsible for designing a decision policy to solve the following task: 
The agent has to pick up a box which is placed in another room, behind a locked door. This environment can be solved without relying on language.


You will write a python `Policy()`, which should be initializable without any parameters from the user, object which has two methods:
- `def act(observation)` which takes in an observation and returns an action.
- `update(observation, action, reward, next_observation)` which takes in the current observation, chosen action, reward, and next_observation and updates any persistent memory/state between observations. You should never assume the actions you take. `update` is a good place to test your understanding of the world and record the results.
The observation space is defined formally as: 
You can see a (7, 7) square of tiles in the direction you are facing and your inventory of item. Formally - observation['agent']['direction'] with 0: right, 1: down, 2: left, 3: up
- observation['agent']['image'] array with shape (7, 7, 3) with each tile in the (7, 7) grid encoded as the triple (object: int, color: int, state: int) where
    - object with 0: unseen, 1: empty, 2: wall, 3: floor, 4: door, 5: key, 6: ball, 7: box, 8: goal, 9: lava
    - color with 0: red, 1: green, 2: blue, 3: purple, 4: yellow, 5: grey
    - state with 0: door open, 1: door closed, 2: door locked
- observation['inv']: list[int] contains any object being held and is empty otherwise 
Note, the agent is always located at observation['image'][3][6] with observation['image'][2] to the left and observation['image'][4] to the right and observation['image'][3][5] forward.


The action space is defined formally as:
action: int such that
- 0: turn left
- 1: turn right
- 2: move forward, Precondition: Forward tile must be empty
- 3: pickup item, Precondition: must be standing in tile adjacent to object and facing it (item must be on observation['image'][3][5]). Cannot be holding another object
- 4: drop item, Precondition: Must be holding item. Tile you are facing must be empty
- 5: toggle key to open door, Precondition: Must have key and be facing the door (door is on observation['image'][3][5])


The rewards are defined formally as:
A reward of ‘1 - 0.9 * (step_count / 300)’ is given for success  for picking up the box
+0.1 for picking up the key for the first time. 
+0.2 for opening the door, and +0.1 for going through the door, 
+0.1 for putting down the key after opening the door.



Here is an example policy:

```python
import numpy as np



class Policy:

    def __init__(self):

        self.memory = {'key_picked_up': False, 'door_opened': False, 'door_opening': False, 'door_unlocked': False, 

                       'box_picked_up': False}  # Persistent memory/state

        

        self.steps = 0

        

        self.actions = [0, 1, 2, 3, 4, 5]  # Possible actions

        self.explore_count = 0  # Count for exploration actions

        

        self.explored_tiles = set()  # To keep track of tiles explored

        

    def act(self, observation):

        direction = observation['agent']['direction']

        image = observation['agent']['image']

        inv = observation['inv']

        

        current_tile = image[3][6]  # Current tile agent is standing on

        forward_tile = image[3][5]  # Tile agent is facing

        

        if forward_tile[0] == 5 and 3 not in inv:

            return 3  # Pickup key

        

        if forward_tile[0] == 4 and forward_tile[2] == 2 and 5 in inv and not self.memory['door_opening']:

            return 5  # Toggle key to open door

        

        if forward_tile[0] == 7 and 0 in inv:

            return 3  # Pickup box if available

        

        if forward_tile[0] not in [2, 9] and tuple(forward_tile) not in self.explored_tiles:

            self.explored_tiles.add(tuple(forward_tile))  # Add the tile to explored set

            return 2  # Move forward if facing an unexplored tile

        

        # Exploration strategy

        self.explore_count += 1

        

        if self.explore_count % 3 == 0:

            return np.random.choice([0, 1])  # Randomly turn left or right every 3rd step

        

        if self.explore_count % 2 == 0:

            return 2  # Move forward every 2nd step

        

        return 2  # Move forward by default

        

    def update(self, observation, action, reward, next_observation):

        key_picked_up = self.memory['key_picked_up']

        door_opened = self.memory['door_opened']

        door_unlocked = self.memory['door_unlocked']

        door_opening = self.memory['door_opening']

        box_picked_up = self.memory['box_picked_up']

        

        self.steps += 1

        

        if not key_picked_up and reward == 0.1:

            self.memory['key_picked_up'] = True

        if not door_unlocked and reward == 0.3:

            self.memory['door_unlocked'] = True

        if not door_opening and reward == 0.2:

            self.memory['door_opening'] = True

        if not door_opened and reward == 0.1:

            self.memory['door_opened'] = True

        if not box_picked_up and reward == 0.9:

            self.memory['box_picked_up'] = True```


The above policy has been rolled out in the environment and a dataset of trajectories has been collected.
You must now analyze this dataset to better understand the policy's behavior and environment dynamics.
This is done by using reporting functions which capture parts of the policy's behavior and the environmental dynamics.

Report:

{
  "num_trajectories": {
    "result": 20,
    "description": "Number of evaluation rollouts."
  },
  "avg_trajectory_reward": {
    "result": 0.4000000000000001,
    "description": "Average reward across trajectories."
  },
  "success_rate": {
    "result": 0.0,
    "description": "Success rate of picking up the box"
  },
  "key_pickup_success": {
    "result": 1.0,
    "description": "Key pickup success rate"
  },
  "door_interaction_efficiency": {
    "result": 1.0,
    "description": "Door interaction efficiency"
  },
  "exploration_efficiency": {
    "result": 0.005186920905748305,
    "description": "Exploration efficiency"
  },
  "report_box_pickup_attempts": {
    "result": 0.0,
    "description": "Number of attempts made to pick up the box."
  },
  "report_steps_before_box_pickup": {
    "result": -1.0,
    "description": "Average number of steps taken before the first attempt to pick up the box."
  },
  "report_distance_traveled_before_box_pickup": {
    "result": 0.0,
    "description": "Distance traveled before attempting to pick up the box."
  },
  "report_efficiency_of_movement": {
    "result": 1.0,
    "description": "Efficiency of movement."
  }
}

Given the above reports, develop a hypothesis for how the policy is failing.