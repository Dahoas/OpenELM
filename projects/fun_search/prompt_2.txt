You are responsible for designing a decision policy to solve the following task: 
The agent has to pick up a box which is placed in another room, behind a locked door. This environment can be solved without relying on language.


You will write a python `Policy()`, which should be initializable without any parameters from the user, object which has two methods:
- `def act(observation)` which takes in an observation and returns an action.
- `update(observation, action, reward, next_observation)` which takes in the current observation, chosen action, reward, and next_observation and updates any persistent memory/state between observations. You should never assume the actions you take. `update` is a good place to test your understanding of the world and record the results.
The observation space is defined formally as: 
You can see a (7, 7) square of tiles in the direction you are facing and your inventory of item. Formally - observation['agent']['direction'] with 0: right, 1: down, 2: left, 3: up
- observation['agent']['image'] array with shape (7, 7, 3) with each tile in the (7, 7) grid encoded as the triple (object: int, color: int, state: int) where
    - object with 0: unseen, 1: empty, 2: wall, 3: floor, 4: door, 5: key, 6: ball, 7: box, 8: goal, 9: lava
    - color with 0: red, 1: green, 2: blue, 3: purple, 4: yellow, 5: grey
    - state with 0: door open, 1: door closed, 2: door locked
- observation['inv']: list[int] contains any object being held and is empty otherwise 
Note, the agent is always located at observation['image'][3][6] with observation['image'][2] to the left and observation['image'][4] to the right and observation['image'][3][5] forward.


The action space is defined formally as:
action: int such that
- 0: turn left
- 1: turn right
- 2: move forward, Precondition: Forward tile must be empty
- 3: pickup item, Precondition: must be standing in tile adjacent to object and facing it (item must be on observation['image'][3][5]). Cannot be holding another object
- 4: drop item, Precondition: Must be holding item. Tile you are facing must be empty
- 5: toggle key to open door, Precondition: Must have key and be facing the door (door is on observation['image'][3][5])


The rewards are defined formally as:
A reward of ‘1 - 0.9 * (step_count / 300)’ is given for success  for picking up the box
+0.1 for picking up the key for the first time. 
+0.2 for opening the door, and +0.1 for going through the door, 
+0.1 for putting down the key after opening the door.



Here is an example policy:

```python
import numpy as np



class Policy:

    def __init__(self):

        self.memory = {'key_picked_up': False, 'door_opened': False, 'door_opening': False, 'door_unlocked': False, 

                       'box_picked_up': False}  # Persistent memory/state

        

        self.steps = 0

        

        self.actions = [0, 1, 2, 3, 4, 5]  # Possible actions

        self.explore_count = 0  # Count for exploration actions

        

        self.explored_tiles = set()  # To keep track of tiles explored

        

    def act(self, observation):

        direction = observation['agent']['direction']

        image = observation['agent']['image']

        inv = observation['inv']

        

        current_tile = image[3][6]  # Current tile agent is standing on

        forward_tile = image[3][5]  # Tile agent is facing

        

        if forward_tile[0] == 5 and 3 not in inv:

            return 3  # Pickup key

        

        if forward_tile[0] == 4 and forward_tile[2] == 2 and 5 in inv and not self.memory['door_opening']:

            return 5  # Toggle key to open door

        

        if forward_tile[0] == 7 and 0 in inv:

            return 3  # Pickup box if available

        

        if forward_tile[0] not in [2, 9] and tuple(forward_tile) not in self.explored_tiles:

            self.explored_tiles.add(tuple(forward_tile))  # Add the tile to explored set

            return 2  # Move forward if facing an unexplored tile

        

        # Exploration strategy

        self.explore_count += 1

        

        if self.explore_count % 3 == 0:

            return np.random.choice([0, 1])  # Randomly turn left or right every 3rd step

        

        if self.explore_count % 2 == 0:

            return 2  # Move forward every 2nd step

        

        return 2  # Move forward by default

        

    def update(self, observation, action, reward, next_observation):

        key_picked_up = self.memory['key_picked_up']

        door_opened = self.memory['door_opened']

        door_unlocked = self.memory['door_unlocked']

        door_opening = self.memory['door_opening']

        box_picked_up = self.memory['box_picked_up']

        

        self.steps += 1

        

        if not key_picked_up and reward == 0.1:

            self.memory['key_picked_up'] = True

        if not door_unlocked and reward == 0.3:

            self.memory['door_unlocked'] = True

        if not door_opening and reward == 0.2:

            self.memory['door_opening'] = True

        if not door_opened and reward == 0.1:

            self.memory['door_opened'] = True

        if not box_picked_up and reward == 0.9:

            self.memory['box_picked_up'] = True```

Average Policy Return: 0.4
Num Policy Rollouts: 100\n\n\
Max rollout length: 300\n\n\

The following metrics have been collected on the policy:

class Metrics:
    def metrics(self, trajectories):
        success_count = 0
        total_rewards = 0
        door_interactions = 0
        key_pickups = 0
        useless_actions = 0
        box_interactions = 0
        direction_changes = 0
        steps_post_door = 0
        distances_to_box_post_door = []

        for trajectory in trajectories:
            rewards = trajectory['rewards']
            actions = trajectory['actions']
            observations = trajectory['observations']

            total_rewards += sum(rewards)
            if rewards[-1] > 0:  # Assuming positive reward only on success
                success_count += 1

            last_direction = observations[0]['agent']['direction']
            door_opened = False
            steps_since_door_opened = 0
            min_distance_to_box_after_door = float('inf')

            for i, (action, reward) in enumerate(zip(actions, rewards)):
                current_obs = observations[i]
                next_obs = observations[i + 1]

                # Check for door interaction
                if current_obs['agent']['image'][3][5][0] == 4 and action == 5:
                    door_interactions += 1
                    door_opened = True  # Assuming the action succeeded

                # Check for key pickups
                if current_obs['agent']['image'][3][5][0] == 5 and action == 3:
                    key_pickups += 1

                # Check for box interactions
                if current_obs['agent']['image'][3][5][0] == 7 and action == 3:
                    box_interactions += 1

                # Check for direction changes
                if action == 0 or action == 1:
                    if last_direction != next_obs['agent']['direction']:
                        direction_changes += 1
                        last_direction = next_obs['agent']['direction']

                # Check for useless actions
                if reward == 0 and (current_obs == next_obs or action in [0, 1, 4]):
                    useless_actions += 1

                # Track steps and distance to box after door is opened
                if door_opened:
                    steps_since_door_opened += 1
                    # Assuming we can calculate distance to the box (this part is pseudo-code)
                    # distance_to_box = calculate_distance_to_box(next_obs)
                    # min_distance_to_box_after_door = min(min_distance_to_box_after_door, distance_to_box)

            if door_opened:
                steps_post_door += steps_since_door_opened
                # distances_to_box_post_door.append(min_distance_to_box_after_door)

        success_rate = success_count / len(trajectories) if trajectories else 0
        average_reward = total_rewards / len(trajectories) if trajectories else 0
        # average_distance_to_box_post_door = sum(distances_to_box_post_door) / len(distances_to_box_post_door) if distances_to_box_post_door else 0

        return {
            'Success Rate': success_rate,
            'Average Reward': average_reward,
            'Door Interactions': door_interactions,
            'Key Pickups': key_pickups,
            'Useless Actions': useless_actions,
            'Box Interactions': box_interactions,
            'Direction Changes': direction_changes,
            'Average Steps After Door Open': steps_post_door / door_interactions if door_interactions else 0,
            # 'Average Distance to Box Post-Door': average_distance_to_box_post_door
        }

The above policy has been rolled out in the environment and a dataset of trajectories has been collected.
You must now analyze this dataset to better understand the policy's behavior and environment dynamics.
This is done by using reporting functions which capture parts of the policy's behavior and the environmental dynamics.

{
  "num_trajectories": {
    "result": 20,
    "description": "Number of evaluation rollouts."
  },
  "avg_trajectory_reward": {
    "result": 0.4000000000000001,
    "description": "Average reward across trajectories."
  },
  "success_rate": {
    "result": 0.0,
    "description": "Success rate of picking up the box"
  },
  "key_pickup_success": {
    "result": 1.0,
    "description": "Key pickup success rate"
  },
  "door_interaction_efficiency": {
    "result": 1.0,
    "description": "Door interaction efficiency"
  },
  "exploration_efficiency": {
    "result": 0.005186920905748305,
    "description": "Exploration efficiency"
  }
}


You must now analyze this dataset to better understand the policy's behavior and environment dynamics.
You should focus on trying to identify policy failure modes or failures to understand the 
environment dynamics. 

If you can confidently identify the cause of a major policy failure case and can
improve the given policy to fix this, write POLICY_IMPROVEMENT on the last line.
Otherwise if you need to collect more metric information, propose more metrics to 
better understand how the policy is behaving.

