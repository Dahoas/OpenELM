{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomohirosawada/Library/Mobile Documents/com~apple~CloudDocs/local/now/o0124/baseline/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# imports \n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json \n",
    "from typing import List, Dict, Union\n",
    "from pathlib import Path, PosixPath\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "# import openai \n",
    "import litellm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alex's API key \n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = OAI_API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper for API\n",
    "\n",
    "# model names \n",
    "\n",
    "# MODEL_NAMES = defaultdict(lambda: None)\n",
    "# MODEL_NAMES.update({\n",
    "#     \"gpt-4\": None, \n",
    "#     \"turbo\": None, \n",
    "#     \"davinci\": None,\n",
    "#     \"chatgpt-3.5\": None \n",
    "# })\n",
    "\n",
    "# class \n",
    "\n",
    "class Model: \n",
    "    \"\"\"\n",
    "    wrapper class for LLM APIs (for now, just gpt 4 and 3.5). \n",
    "    we will design this API so that it is as easy as possible to conduct experiments. \n",
    "    \n",
    "    DETAILS: \n",
    "    1.) iid sampling: we will prompt the model with the same prompt multiple times and store the outputs in a jsonl file \n",
    "    2.) iterative self-improvement: we will prompt the model with the same prompt, but will also ask the model to improve upon its previous output.\n",
    "\n",
    "    3.) (TODO later) run evaluations: there is a script policy_vis.py for evaluating model performance. We will build this into the API so that it is very easy to evaluate. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                model: str, \n",
    "                temp: int = 0.7, \n",
    "                model_name: str = None , \n",
    "                prompt: Union[str,Path] = None, \n",
    "                self_improv: Union[str,Path] = None, \n",
    "                save_path: Union[str,Path] = None):\n",
    "\n",
    "        # model configuration\n",
    "        self.oai_api: str = OAI_API\n",
    "        self.model_name: str = model_name if model_name else model\n",
    "        self.model: str = model # MODEL_NAMES[self.model_name]\n",
    "        self.temp: int = temp\n",
    "        self.timestamp: str = None # lastrun \n",
    "\n",
    "        # prompting \n",
    "        self.prompt = prompt if type(prompt) == str else open(prompt).read()\n",
    "        self.sample_size: int = 5 # TODO: set this to 100 for full experiment\n",
    "        self.token_limit: int = 100 # TODO: set this to 1000 for full experiment\n",
    "\n",
    "        self.self_improv_prompt = self_improv if type(self_improv) == str else open(self_improv).read()\n",
    "        self.improv_count: int = 2 # TODO: allow larger values for full experiment\n",
    "\n",
    "        # saving results \n",
    "        self.save_path: Path = Path(save_path) if type(save_path) == str else save_path\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        \"\"\"\n",
    "        Pretty print the following information:\n",
    "        - model name\n",
    "        - temp\n",
    "        - token limit\n",
    "        - sample size\n",
    "        - prompt (first 20 chars followed by ...)\n",
    "        - self improv prompt (first 20 chars followed by ...)\n",
    "        \"\"\"\n",
    "        prompt_preview = (self.prompt[:20] + '...') if self.prompt else 'None'\n",
    "        improv_preview = (self.self_improv_prompt[:20] + '...') if self.self_improv_prompt else 'None'\n",
    "        return (f\"Model(name={self.model_name}, temp={self.temp}, token_limit={self.token_limit}, \"\n",
    "                f\"sample_size={self.sample_size}, prompt={prompt_preview}, \"\n",
    "                f\"self_improv_prompt={improv_preview})\")\n",
    "\n",
    "    # utility functions\n",
    "    def _run_completion(self, prompt, temp, token_limit):\n",
    "        response = litellm.completion(\n",
    "            model=self.model,\n",
    "            messages=[{\"content\": prompt, \"role\": \"user\"}],\n",
    "            max_tokens=token_limit,\n",
    "            temperature=temp\n",
    "        )\n",
    "        # Check if response is dictionary type or an object with attributes\n",
    "        if isinstance(response, dict):\n",
    "            # Dictionary access\n",
    "            message_content = response['choices'][0]['message']['content']\n",
    "        else:\n",
    "            # Object attribute access\n",
    "            message_content = response.choices[0].message.content\n",
    "        # Return the message content directly, or adapt this part as needed\n",
    "        return message_content\n",
    "\n",
    "\n",
    "    def _save_result_to_file(self, filename: str, result):\n",
    "        \"\"\"\n",
    "        Save a single result to a file.\n",
    "        \"\"\"\n",
    "        with open(filename, 'a') as file:  # Open in append mode\n",
    "            file.write(str({\"result\": json.dumps(result)}) + \"\\n\")  # Write result as a JSON line\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def completion(self, prompt: Union[str, Path] = None, sample_size: int = None, temp: int = None, token_limit: int = None): \n",
    "        \"\"\"\n",
    "        function for getting model output given a prompt.\n",
    "        \"\"\"\n",
    "        \n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            final_prompt = prompt\n",
    "        elif prompt is not None and isinstance(prompt, Path):\n",
    "            final_prompt = prompt.read_text()\n",
    "        elif self.prompt: final_prompt = self.prompt \n",
    "        else: \n",
    "            raise ValueError(\"Prompt can't be Non.\")\n",
    "\n",
    "        temperature = temp if temp is not None else self.temp\n",
    "        max_tokens = token_limit if token_limit is not None else self.token_limit\n",
    "        self.timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "        if sample_size is None or sample_size <= 1:\n",
    "            response = self._run_completion(final_prompt, temperature, max_tokens)\n",
    "            if self.save_path:\n",
    "                filename = f\"{self.save_path}/results_{self.model_name}_single_{self.timestamp}_1_{temp}_{token_limit}.jsonl\"\n",
    "                self._save_result_to_file(filename, response)\n",
    "            return [response]\n",
    "        else:\n",
    "            experiment_type = \"iid\"\n",
    "            filename = f\"{self.save_path}/results_{self.model_name}_{experiment_type}_{self.timestamp}_{sample_size}_{temp}_{token_limit}.jsonl\"\n",
    "            for _ in range(sample_size):\n",
    "                response = self._run_completion(final_prompt, temperature, max_tokens)\n",
    "                self._save_result_to_file(filename, response)\n",
    "            return None  # Or return an indication that results have been saved.\n",
    "\n",
    "    \n",
    "\n",
    "    def self_improv(self, prompt: str = None, self_improv_prompt: str = None, improv_count: int = None, sample_count: int = 1, temp: int = None, token_limit: int = None, final_result_only: bool = True):\n",
    "        \"\"\"\n",
    "        Function for running iterative self-improvement with sampling.\n",
    "\n",
    "        Runs self-improvement by first prompting the model and then asking it to improve upon its previous output. Samples the improvement step multiple times and optionally saves the results to a jsonl file. Use temp and token_limit to control the sampling. Run it improv_count times with sample_count samples at each step.\n",
    "\n",
    "        If final_result_only is False, saves all responses to a JSON file. Otherwise, only saves the final improved output.\n",
    "        \"\"\"\n",
    "        final_prompt = prompt if prompt else self.self_improv_prompt\n",
    "        improv_iterations = improv_count if improv_count is not None else self.improv_count\n",
    "        temperature = temp if temp is not None else self.temp\n",
    "        max_tokens = token_limit if token_limit is not None else self.token_limit\n",
    "        \n",
    "        all_outputs = []  # Collect all outputs here\n",
    "            \n",
    "        # Perform initial completion to start the self-improvement process\n",
    "        improved_output = self._run_completion(final_prompt, temperature, max_tokens)\n",
    "        \n",
    "        # Iteratively ask the model to improve upon the previous output, sampling each step\n",
    "        for _ in range(improv_iterations - 1):\n",
    "            new_prompts = [f\"Improve upon the following: {improved_output}\" for _ in range(sample_count)]\n",
    "            improved_outputs = [self._run_completion(new_prompt, temperature, max_tokens) for new_prompt in new_prompts]\n",
    "            all_outputs.extend(improved_outputs)  # Save outputs from each iteration\n",
    "        \n",
    "        # Optionally, save outputs to a file\n",
    "        if self.save_path:\n",
    "            file_name_template = \"results_{model_name}_self_improv_{timestamp}_{sample_size}_{temp}_{token_limit}.jsonl\"\n",
    "            experiment_type = \"self_improv\"\n",
    "            results_to_save = [{\"result\": improved_output}] if final_result_only else [{\"prompt\": final_prompt, \"result\": output} for output in all_outputs]\n",
    "            \n",
    "            self._save_results_to_file(file_name_template, results_to_save, experiment_type, sample_count, temp, token_limit)\n",
    "        \n",
    "        return improved_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(template: Union[str, PosixPath], task_descr: Union[Dict[str, str], PosixPath], prompt_path: PosixPath = Path(\"prompt.txt\")) -> str:\n",
    "    \"\"\"\n",
    "    Fill in template (given as f-string) with the fields in the task description. \n",
    "    Print out result as well as save to path.\n",
    "    \"\"\"\n",
    "    if isinstance(template, PosixPath):\n",
    "        template = template.read_text()\n",
    "    if isinstance(task_descr, PosixPath):\n",
    "        task_descr = task_descr.read_text()  # Assuming JSON or similar, needs parsing\n",
    "\n",
    "    filled_template = template.format(**task_descr)\n",
    "    print(filled_template)\n",
    "    with open(prompt_path, 'w') as file:\n",
    "        file.write(filled_template)\n",
    "\n",
    "    return filled_template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement evaluation function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Template for the prompt\n",
    "template = \"\"\"\\\n",
    "You are responsible for designing a value function to solve the following task: \n",
    "{task_description}\\n\\n\\\n",
    "You will write a python `Value`, which should be initializable without any parameters from the user, object which has one method:\n",
    "- `def value(observation)` which takes in an observation and returns the value of the observation. The \\\n",
    "output should be normalized between -1 and 1.\n",
    "Note: You should not assume any exploration outside of what is learned during the agent's single rollout in \\\n",
    "the environment. This means you should not rely on Q-learning, requiring extra exploration.\\n\\n\\\n",
    "The observation space is defined formally as: \n",
    "{observation_description}\\n\\n\\\n",
    "You are allowed to use any python library you want but should not assume access \\\n",
    "to any other external resources (such as models with downloadable weights) unless otherwise specified. \\\n",
    "In particular you can assume access to the following APIs: \\\n",
    "{api_description}\\n\\n\\\n",
    "You should only write the Value class and nothing else. \\\n",
    "You are encouraged to be as creative as possible, do not simply copy one of the exemplars if given. \\\n",
    "All code should be written in a single, large code block.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "    \"chess\": dict( \n",
    "        task_description=\"\"\"\n",
    "        You are a chess world champion. Win the chess game. You are playing white. You have no time constraints.  \n",
    "\"\"\", \n",
    "        observation_description=\"\"\"\n",
    "        observation: chess.Boards() object from the python-chess library. It has the following attributes which may be useful:\n",
    "        \n",
    "        move_stack: List[Move]\n",
    "        The move stack. Use Board.push(), Board.pop(), Board.peek() and Board.clear_stack() for manipulation.\n",
    "\n",
    "        propertylegal_moves: LegalMoveGenerator\n",
    "        A dynamic list of legal moves.\n",
    "        Note: To use this as a list you will need to do list(board.legal_moves)\n",
    "\n",
    "        import chess\n",
    "\n",
    "        board = chess.Board()\n",
    "        board.legal_moves.count()\n",
    "        20\n",
    "        bool(board.legal_moves)\n",
    "        True\n",
    "        move = chess.Move.from_uci(\"g1f3\")\n",
    "        move in board.legal_moves\n",
    "        True\n",
    "        Wraps generate_legal_moves() and is_legal().\n",
    "\n",
    "\n",
    "        checkers()→ SquareSet[source]\n",
    "        Gets the pieces currently giving check.\n",
    "\n",
    "        Returns a set of squares.\n",
    "\n",
    "\n",
    "        is_check()→ bool[source]\n",
    "        Tests if the current side to move is in check.\n",
    "\n",
    "        gives_check(move: Move)→ bool[source]\n",
    "        Probes if the given move would put the opponent in check. The move must be at least pseudo-legal.\n",
    "\n",
    "\n",
    "        is_checkmate()→ bool[source]\n",
    "        Checks if the current position is a checkmate.\n",
    "\n",
    "        is_stalemate()→ bool[source]\n",
    "        Checks if the current position is a stalemate.\n",
    "\n",
    "        is_insufficient_material()→ bool[source]\n",
    "        Checks if neither side has sufficient winning material (has_insufficient_material()).\n",
    "\n",
    "        has_insufficient_material(color: chess.Color)→ bool[source]\n",
    "        Checks if color has insufficient winning material.\n",
    "\n",
    "        This is guaranteed to return False if color can still win the game.\n",
    "\n",
    "        The converse does not necessarily hold: The implementation only looks at the material, including the colors of bishops, but not considering piece positions. So fortress positions or positions with forced lines may return False, even though there is no possible winning line.\n",
    "\n",
    "\n",
    "        find_move(from_square: chess.Square, to_square: chess.Square, promotion: chess.PieceType | None = None)→ Move[source]\n",
    "        Finds a matching legal move for an origin square, a target square, and an optional promotion piece type.\n",
    "\n",
    "        For pawn moves to the backrank, the promotion piece type defaults to chess.QUEEN, unless otherwise specified.\n",
    "\n",
    "        Castling moves are normalized to king moves by two steps, except in Chess960.\n",
    "\n",
    "        Raises\n",
    "        :\n",
    "        IllegalMoveError if no matching legal move is found.\n",
    "\"\"\", \n",
    "        api_description=\"\"\"\n",
    "        None\n",
    "\"\"\", \n",
    "        action_description=\"\"\"\n",
    "        None\n",
    "\"\"\",\n",
    "        reward_description=\"\"\"\n",
    "        None\n",
    "\"\"\",\n",
    "        action_exemplar=\"\"\"\n",
    "        None\n",
    "\"\"\"\n",
    "    ), \n",
    "\n",
    "    \"Blackjack-v1\": dict(\n",
    "        task_description=\"\"\"Win the blackjack hand. Each round will be separate, independent of the ones before.\n",
    "\"\"\",\n",
    "        observation_description=\"\"\"observation: Tuple[int, int, bool] where:\\n\\\n",
    "observation[0] = The sum of your cards\\n\\\n",
    "observation[1] = The dealer's showing card sum.\\n\\\n",
    "observation[2] = True if you have an Ace, False otherwise\\n\\\n",
    "\"\"\",\n",
    "     action_description=\"\"\"action: str \\n\\\n",
    "chess notation for player1's move. \\n\\\n",
    "\"\"\",\n",
    "        reward_description=\"\"\"A reward of between -1 and 1 is given for each move. Takes into account: number of pieces captured, piece mobility, pawn structure\n",
    "         , king safety, control of center of board, how well balanced \n",
    "           the pieces are around the board.  \\n\\\n",
    "\"\"\",\n",
    "        action_exemplar=\"\"\"\\\n",
    "- a_1 = \"e2e4\"  # Player 1 moves pawn from e2 to e4\n",
    "- a_2 = \"e7e5\"  # Player 2 moves pawn from e7 to e5\n",
    "- a_3 = \"g1f3\"  # Player 1 moves knight from g1 to f3\n",
    "- a_4 = \"b8c6\"  # Player 2 moves knight from b8 to c6\n",
    "\"\"\",\n",
    "    ),\n",
    "\n",
    "    \"MiniGrid-BlockedUnlockPickup-v0\": dict(\n",
    "        task_description=\"\"\"You are an agent in 2-D gridworld. The agent has to pick up a box which is placed in another room, behind a locked door. \\\n",
    "The door is also blocked by a ball which the agent has to move before it can unlock the door. \\\n",
    "Hence, the agent has to learn to move the ball, pick up the key, open the door and pick up the object in the other room.\n",
    "\"\"\",\n",
    "        observation_description=\"\"\"You can only see a (7, 7) square of tiles in the direction you are facing. \\\n",
    "Formally `observation: Dict('direction': Discrete(4), 'image':  array: (7, 7, 3)))` \\\n",
    "where:\n",
    "- observation['direction'] with 0: right, 1: down, 2: left, 3: up\\n\\\n",
    "- observation['image'] array with shape (7, 7, 3) with each tile in the (7, 7) grid encoded as the triple (object: int, color: int, state: int) where\n",
    "    - object with 0: unseen, 1: empty, 2: wall, 3: floor, 4: door, 5: key, 6: ball, 7: box, 8: goal, 9: lava\n",
    "    - color with 0: red, 1: green, 2: blue, 3: purple, 4: yellow, 5: grey\n",
    "    - state with 0: door open, 1: door closed, 2: door locked\n",
    "Note, the agent is always located at observation['image'][3][6] with \\\n",
    "observation['image'][2] to the left and observation['image'][4] to the right and observation['image'][3][5] forward\n",
    "\"\"\",\n",
    "        action_description=\"\"\"action: int such that\\n\\\n",
    "- 0: turn left\\n\\\n",
    "- 1: turn right\\n\\\n",
    "- 2: move forward\\n\\\n",
    "- 3: pickup item\\n\n",
    "\"\"\",\n",
    "        reward_description=\"\"\"A reward of ‘1 - 0.9 * (step_count / max_steps)’ is given for success, and ‘0’ for failure.\n",
    "\"\"\",\n",
    "        action_exemplar=\"\"\"\\\n",
    "- a_1 = 1  # I don't see anything so turn right\n",
    "- a_2 = 2  # I see the key to my forward left and the door to my right so I walk toward both\n",
    "- a_3 = 2  # Walk forward again toward key\n",
    "- a_4 = 0  # Turn toward key\n",
    "- a_5 = 3  # pickup key\n",
    "- a_6 = 1  # Turn right to pickup ball to my right\n",
    "- a_7 = 2  # Walk toward ball\n",
    "- a_8 = 3  # pickup ball in front of me\n",
    "- a_9 = 2  # Walk forward towards door\n",
    "- a_10 = 2  # Walk through door with key\n",
    "- a_11 = 2  # Walk forward\n",
    "- a_12 = 1  # I see the chest to my right so I turn right\n",
    "- a_13 = 2  # Walk toward chest\n",
    "- a_14 = 3  # Pickup chest\n",
    "\"\"\",\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are responsible for designing a value function to solve the following task: \n",
      "\n",
      "        You are a chess world champion. Win the chess game. You are playing white. You have no time constraints.  \n",
      "\n",
      "\n",
      "You will write a python `Value`, which should be initializable without any parameters from the user, object which has one method:\n",
      "- `def value(observation)` which takes in an observation and returns the value of the observation. The output should be normalized between -1 and 1.\n",
      "Note: You should not assume any exploration outside of what is learned during the agent's single rollout in the environment. This means you should not rely on Q-learning, requiring extra exploration.\n",
      "\n",
      "The observation space is defined formally as: \n",
      "\n",
      "        observation: chess.Boards() object from the python-chess library. It has the following attributes which may be useful:\n",
      "        \n",
      "        move_stack: List[Move]\n",
      "        The move stack. Use Board.push(), Board.pop(), Board.peek() and Board.clear_stack() for manipulation.\n",
      "\n",
      "        propertylegal_moves: LegalMoveGenerator\n",
      "        A dynamic list of legal moves.\n",
      "        Note: To use this as a list you will need to do list(board.legal_moves)\n",
      "\n",
      "        import chess\n",
      "\n",
      "        board = chess.Board()\n",
      "        board.legal_moves.count()\n",
      "        20\n",
      "        bool(board.legal_moves)\n",
      "        True\n",
      "        move = chess.Move.from_uci(\"g1f3\")\n",
      "        move in board.legal_moves\n",
      "        True\n",
      "        Wraps generate_legal_moves() and is_legal().\n",
      "\n",
      "\n",
      "        checkers()→ SquareSet[source]\n",
      "        Gets the pieces currently giving check.\n",
      "\n",
      "        Returns a set of squares.\n",
      "\n",
      "\n",
      "        is_check()→ bool[source]\n",
      "        Tests if the current side to move is in check.\n",
      "\n",
      "        gives_check(move: Move)→ bool[source]\n",
      "        Probes if the given move would put the opponent in check. The move must be at least pseudo-legal.\n",
      "\n",
      "\n",
      "        is_checkmate()→ bool[source]\n",
      "        Checks if the current position is a checkmate.\n",
      "\n",
      "        is_stalemate()→ bool[source]\n",
      "        Checks if the current position is a stalemate.\n",
      "\n",
      "        is_insufficient_material()→ bool[source]\n",
      "        Checks if neither side has sufficient winning material (has_insufficient_material()).\n",
      "\n",
      "        has_insufficient_material(color: chess.Color)→ bool[source]\n",
      "        Checks if color has insufficient winning material.\n",
      "\n",
      "        This is guaranteed to return False if color can still win the game.\n",
      "\n",
      "        The converse does not necessarily hold: The implementation only looks at the material, including the colors of bishops, but not considering piece positions. So fortress positions or positions with forced lines may return False, even though there is no possible winning line.\n",
      "\n",
      "\n",
      "        find_move(from_square: chess.Square, to_square: chess.Square, promotion: chess.PieceType | None = None)→ Move[source]\n",
      "        Finds a matching legal move for an origin square, a target square, and an optional promotion piece type.\n",
      "\n",
      "        For pawn moves to the backrank, the promotion piece type defaults to chess.QUEEN, unless otherwise specified.\n",
      "\n",
      "        Castling moves are normalized to king moves by two steps, except in Chess960.\n",
      "\n",
      "        Raises\n",
      "        :\n",
      "        IllegalMoveError if no matching legal move is found.\n",
      "\n",
      "\n",
      "You are allowed to use any python library you want but should not assume access to any other external resources (such as models with downloadable weights) unless otherwise specified. In particular you can assume access to the following APIs: \n",
      "        None\n",
      "\n",
      "\n",
      "You should only write the Value class and nothing else. You are encouraged to be as creative as possible, do not simply copy one of the exemplars if given. All code should be written in a single, large code block.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'You are responsible for designing a value function to solve the following task: \\n\\n        You are a chess world champion. Win the chess game. You are playing white. You have no time constraints.  \\n\\n\\nYou will write a python `Value`, which should be initializable without any parameters from the user, object which has one method:\\n- `def value(observation)` which takes in an observation and returns the value of the observation. The output should be normalized between -1 and 1.\\nNote: You should not assume any exploration outside of what is learned during the agent\\'s single rollout in the environment. This means you should not rely on Q-learning, requiring extra exploration.\\n\\nThe observation space is defined formally as: \\n\\n        observation: chess.Boards() object from the python-chess library. It has the following attributes which may be useful:\\n        \\n        move_stack: List[Move]\\n        The move stack. Use Board.push(), Board.pop(), Board.peek() and Board.clear_stack() for manipulation.\\n\\n        propertylegal_moves: LegalMoveGenerator\\n        A dynamic list of legal moves.\\n        Note: To use this as a list you will need to do list(board.legal_moves)\\n\\n        import chess\\n\\n        board = chess.Board()\\n        board.legal_moves.count()\\n        20\\n        bool(board.legal_moves)\\n        True\\n        move = chess.Move.from_uci(\"g1f3\")\\n        move in board.legal_moves\\n        True\\n        Wraps generate_legal_moves() and is_legal().\\n\\n\\n        checkers()→ SquareSet[source]\\n        Gets the pieces currently giving check.\\n\\n        Returns a set of squares.\\n\\n\\n        is_check()→ bool[source]\\n        Tests if the current side to move is in check.\\n\\n        gives_check(move: Move)→ bool[source]\\n        Probes if the given move would put the opponent in check. The move must be at least pseudo-legal.\\n\\n\\n        is_checkmate()→ bool[source]\\n        Checks if the current position is a checkmate.\\n\\n        is_stalemate()→ bool[source]\\n        Checks if the current position is a stalemate.\\n\\n        is_insufficient_material()→ bool[source]\\n        Checks if neither side has sufficient winning material (has_insufficient_material()).\\n\\n        has_insufficient_material(color: chess.Color)→ bool[source]\\n        Checks if color has insufficient winning material.\\n\\n        This is guaranteed to return False if color can still win the game.\\n\\n        The converse does not necessarily hold: The implementation only looks at the material, including the colors of bishops, but not considering piece positions. So fortress positions or positions with forced lines may return False, even though there is no possible winning line.\\n\\n\\n        find_move(from_square: chess.Square, to_square: chess.Square, promotion: chess.PieceType | None = None)→ Move[source]\\n        Finds a matching legal move for an origin square, a target square, and an optional promotion piece type.\\n\\n        For pawn moves to the backrank, the promotion piece type defaults to chess.QUEEN, unless otherwise specified.\\n\\n        Castling moves are normalized to king moves by two steps, except in Chess960.\\n\\n        Raises\\n        :\\n        IllegalMoveError if no matching legal move is found.\\n\\n\\nYou are allowed to use any python library you want but should not assume access to any other external resources (such as models with downloadable weights) unless otherwise specified. In particular you can assume access to the following APIs: \\n        None\\n\\n\\nYou should only write the Value class and nothing else. You are encouraged to be as creative as possible, do not simply copy one of the exemplars if given. All code should be written in a single, large code block.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `envs` dictionary is defined elsewhere in the script, or loaded from an external source\n",
    "\n",
    "# Example usage for a specific task, e.g., \"chess\"\n",
    "task_descr = envs[\"chess\"]\n",
    "\n",
    "# Generate prompt for \"chess\" task\n",
    "prompt_path = Path(\"chess_prompt.txt\")  # Define a specific file path if needed\n",
    "generate_prompt(template, task_descr, prompt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4 = Model(model=\"gpt-3.5-turbo\", temp=0.7, prompt=prompt_path, self_improv=\"self_improv.txt\", save_path=\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4.model=\"gpt-3.5-turbo\"\n",
    "gpt4.save_path = Path(\"results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4.completion(prompt = prompt_path, sample_size=100, temp=1, token_limit=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt4.self_improv(prompt = prompt_path, self_improv_prompt=Path(\"self-improv.txt\"), improv_count=2, temp=0.7, token_limit=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
